{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####set up environment\n",
    "\n",
    "import pymssql\n",
    "import pandas as pd\n",
    "conn = pymssql.connect(server='#########', user='#########', password='#########', database='############')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('SELECT REMARKS,EndItemId,Id FROM Da2408_13_1')\n",
    "\n",
    "row = cursor.fetchone()\n",
    "\n",
    "while row:\n",
    "\n",
    "  print(str(row[0]) + \" \" + str(row[1]))\n",
    "\n",
    "  row = cursor.fetchone()\n",
    "\n",
    "\n",
    "q='''SELECT REMARKS,Id,EndItemId, CorrectiveAction, DiscoveryDateTime, CorrectiveDateTime,CorrectiveWorkUnitCode FROM Da2408_13_1'''\n",
    "df1=pd.read_sql(q,conn)\n",
    "df1Sample=df1.sample(20000)\n",
    "df1Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \tREMARKS \tId \tEndItemId \tCorrectiveAction \tDiscoveryDateTime \tCorrectiveDateTime \tCorrectiveWorkUnitCode\n",
    "298088 \tOPERATE IAW THE LIMITATIONS AND RESTRICTIONS S... \t3cdecad9-5800-4d32-98a8-402c2be39307 \t25c4f83e-e60a-5e13-894f-79695829aea1 \tUPDATED AWR BULK ENTRY IAW DA PAM 738-751. \t2021-03-27 08:46:45.897 \t2021-03-31 00:36:00 \t02\n",
    "143208 \tW415 INSP DUE \t909142a6-0628-4a9e-80b2-1fd3faecae6d \t4e3e705c-32ff-4dbe-8673-4063df539e8d \tCOMPLETED IAW TM 1-1520 LONGBOW/APACHE \t2021-04-14 12:24:30.570 \t2021-04-16 09:15:00 \t35A\n",
    "218767 \tR 40 PANEL UPPER ORINGS DETERIORATED \tdf1a5134-0ef7-589c-a37e-94f4137dbbc1 \tbf1c0c9a-ae7c-5828-8d59-5ff20d7e225a \tREPLACED ORINGS IAW TM 1-1520-LONGBOW/APACHE \t2019-02-04 19:03:00.000 \t2019-02-15 15:24:00 \t02\n",
    "475536 \tFMC MOC DUE FOR COMPLETION OF CCI/WASH\\r\\n \t091f89c1-ac55-4c96-ac52-9a09aee2fa12 \taec1adb4-651b-58cb-acf0-cee9213ff3e3 \tCOMPLETED IAW 1-1520-LONGBOW/APACHE \t2020-09-27 07:44:05.123 \t2020-09-30 03:47:00 \t00\n",
    "526159 \tDURING MSPU SURVEY NO.4 DRIVE SHAFT CI YELLOW ... \t9e7c38eb-1ab3-45df-b5c9-f08fff985ef8 \te99d5551-aba4-5ab9-9948-dedd0a6997f3 \tNone \t2021-05-19 14:36:26.750 \tNaT \tNone\n",
    "... \t... \t... \t... \t... \t... \t... \t...\n",
    "216575 \tNIU CHECK DUE FOR COMPLETION OF PMS \t989dec47-d4e2-50cf-a1fd-23d522b566f4 \tbf1c0c9a-ae7c-5828-8d59-5ff20d7e225a \tMOC COMPLETE OK IAW TM 1-1520-LONGBOW/APACHE \t2019-08-16 01:09:00.000 \t2019-08-16 02:40:00 \t10P\n",
    "703 \tPMD Due \teb3883df-50ce-4e8e-a8c8-20423b798a28 \t28fe7469-b0ed-5162-9220-0008631cf16a \tCOMPLETED \t2019-11-20 01:12:00.000 \t2019-11-20 01:16:00 \t00\n",
    "242421 \t24 hour cure time due (or untill cured) for pr... \t7a3df444-32a7-440b-886a-8b13a37ad9d5 \t4920da1d-e6a3-57e9-9e3d-68f8fdde2c1c \tsealant is cured iaw tm 1-1520-longbow/apache \t2020-05-06 13:53:57.477 \t2020-05-07 06:17:00 \t02\n",
    "382678 \tINSP A800 FOLLOWING THE LAST FLIGHT OF THE DAY... \t543e0c04-2e0f-45aa-ade3-418bc2d57113 \t2c6d9a5d-f5e5-5c92-a049-b30bd5856a22 \tcomp iaw awr 2014E-A48 R7 icw pmd dated 29nov2019 \t2019-11-25 22:48:00.000 \t2019-11-29 22:31:00 \t02\n",
    "87026 \tINSP A867 - INSPECT THE MR FEATHER BEARING PRI... \tfcd13395-243f-5317-ba51-884e6ab3c608 \td09d7a51-5cfb-581a-ade7-27a4948e91ed \tCOMP IAW PART 2 OF ADDENDUM 1 PER H-64-19-ASA... \t2019-07-18 19:45:00.000 \t2019-07-19 00:52:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Tokenize text field from data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(term_def.lower()), tags=[list_id[i]]) for i, term_def in enumerate(list_docs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at some example terms\n",
    "model.wv.most_similar(\"engine\")\n",
    "model.wv.most_similar(\"hazard\")\n",
    "model.wv.most_similar(\"emergency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Build Doc2Vec model using tokenized text field \n",
    "max_epochs = 50\n",
    "model = Doc2Vec()  # of course, if non-default parameters needed, use them here\n",
    "\n",
    "                   # but most users won't need to change alpha/min_alpha at all\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data[1:10000], total_examples=model.corpus_count, epochs=max_epochs)\n",
    "model.save(\"d2vSimple.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 2 dimensional space from components \n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "df = pd.DataFrame(X_tsne, index=doc_tags[1:10000], columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###parser function used to generate columns for chart\n",
    "def parser(input_file):\n",
    "\n",
    "    df_dx = pd.DataFrame(columns=['id', 'EndItemId', 'Remark'])\n",
    "\n",
    " \n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "\n",
    "        term_id = term_name = term_def = None\n",
    "\n",
    "        term_is_a = []\n",
    "\n",
    " \n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            line = line.rstrip('\\n')\n",
    "\n",
    " \n",
    "\n",
    "            if \"[Term]\" in line or \"[Typedef]\" in line:            \n",
    "\n",
    "                if term_def:\n",
    "\n",
    "                    df_dx.loc[len(df_dx)] = [term_id, term_name, term_def, term_is_a]\n",
    "\n",
    " \n",
    "\n",
    "                term_id = term_name = term_def = None\n",
    "\n",
    "                term_is_a = []\n",
    "\n",
    "            elif \"id: \" == line[0:4]:\n",
    "\n",
    "                term_id = line.split(\"id: \")[1]\n",
    "\n",
    "            elif \"name: \" in line:\n",
    "\n",
    "                term_name = line.split(\"name: \")[1]\n",
    "\n",
    "            elif \"def: \" in line:\n",
    "\n",
    "                temp = line.split(\"def: \")[1]\n",
    "\n",
    "                term_def = temp.split(\"\\\"\")[1]\n",
    "\n",
    "            elif \"is_a: \" in line:\n",
    "\n",
    "                temp = line.split(\"is_a: \")[1]\n",
    "\n",
    "                temp = temp.split(\" ! \")[0]\n",
    "\n",
    "                term_is_a.append(temp)\n",
    "\n",
    " \n",
    "\n",
    "    return df_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###plot Cluster results, this was done in a Tableau Server\n",
    "def plotScatter(keyword):\n",
    "\n",
    "    fig = plt.figure(figsize=(10,15))\n",
    "\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    " \n",
    "\n",
    "    pos_found_x = []\n",
    "\n",
    "    pos_found_y = []\n",
    "\n",
    "    found_names = []\n",
    "\n",
    " \n",
    "\n",
    "    pos_rest_x = []\n",
    "\n",
    "    pos_rest_y = []\n",
    "\n",
    " \n",
    "\n",
    "    for term_id, pos in df.iterrows():\n",
    "\n",
    "        term_name = df_dx[df_dx['id'] == term_id]['EndID'].values[0]\n",
    "\n",
    "        term_def = df_dx[df_dx['id'] == term_id]['def'].values[0].lower()\n",
    "\n",
    " \n",
    "\n",
    "        if keyword in term_name:\n",
    "\n",
    "            pos_found_x.append(pos['x'])\n",
    "\n",
    "            pos_found_y.append(pos['y'])\n",
    "\n",
    "        elif keyword in term_def:\n",
    "\n",
    "            pos_found_x.append(pos['x'])\n",
    "\n",
    "            pos_found_y.append(pos['y'])\n",
    "\n",
    "        else:\n",
    "\n",
    "            found = False\n",
    "\n",
    "            is_a_list = df_dx[df_dx['id'] == term_id]['is_a'].tolist()[0]\n",
    "\n",
    " \n",
    "\n",
    "            for is_a_id in is_a_list:\n",
    "\n",
    "                if len(df_dx[df_dx['id'] == is_a_id]) > 0:\n",
    "\n",
    "                    if keyword in df_dx[df_dx['id'] == is_a_id]['name'].values[0]:\n",
    "\n",
    "                        pos_found_x.append(pos['x'])\n",
    "\n",
    "                        pos_found_y.append(pos['y'])\n",
    "\n",
    "                        found = True\n",
    "\n",
    "                        break\n",
    "\n",
    "                    elif keyword in df_dx[df_dx['id'] == is_a_id]['def'].values[0].lower():\n",
    "\n",
    "                        pos_found_x.append(pos['x'])\n",
    "\n",
    "                        pos_found_y.append(pos['y'])\n",
    "\n",
    "                        found = True\n",
    "\n",
    "                        break\n",
    "\n",
    " \n",
    "\n",
    "            if found == False:\n",
    "\n",
    "                pos_rest_x.append(pos['x'])\n",
    "\n",
    "                pos_rest_y.append(pos['y']) \n",
    "\n",
    " \n",
    "\n",
    "    ax.scatter(pos_rest_x, pos_rest_y, c='blue')       \n",
    "\n",
    "    ax.scatter(pos_found_x, pos_found_y, c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
